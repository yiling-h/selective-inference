{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45bd4243318f6be0",
   "metadata": {},
   "source": [
    "# Submitting (Serial or Parallel) Jobs on Great Lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355013151c77685f",
   "metadata": {},
   "source": [
    "Submitting jobs onto a cluster can be complicated. To learn about some basics of the structure of Great Lakes, the UM computing cluster, please refer to the [ITS intro slides](https://docs.google.com/presentation/d/1ONH2dwnR75qnJ3RHEK3VQseGkbAWO9pVdBWxl6UoXpc/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5f6d013858081",
   "metadata": {},
   "source": [
    "To configure your jobs in a format to be submitted to the cluster, you need to create a batch script, typically a \"xxx.sbat\" file. An example for a batch script is given below."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ac96099d68fc789",
   "metadata": {},
   "source": [
    "#SBATCH --account=stats_dept1\n",
    "#SBATCH --partition=standard\n",
    "\n",
    "#SBATCH --ntasks=10\n",
    "#SBATCH --nodes=10\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --cpus-per-task=16\n",
    "\n",
    "## 5GB/cpu is the basic share\n",
    "#SBATCH --mem-per-cpu=5GB\n",
    "\n",
    "## wall time days:hours:minutes:seconds\n",
    "#SBATCH --time=00-10:00:00\n",
    "\n",
    "cd /home/yilingh/SI-Graphs\n",
    "source env3/bin/activate\n",
    "module load python/3.10\n",
    "\n",
    "start_end=(0 5 10 15 20 25 30 35 40 45 50)\n",
    "\n",
    "# Get the length of the array\n",
    "array_length=${#start_end[@]}\n",
    "\n",
    "# Loop through the array with an iteration index\n",
    "for ((i=0; i<array_length-1; i++)); do\n",
    "    # Your logic for each iteration here\n",
    "    python3 selectinf/Tests/nbd_simulation_vary_signal.py ${start_end[i]} ${start_end[i+1]} 0 16 &\n",
    "done\n",
    "wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f08a49a4a77d35",
   "metadata": {},
   "source": [
    "Let's now go through these line by line so that you know what to modify when you need to create a batch script for your own purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1186ebc81c21e30",
   "metadata": {},
   "source": [
    "# Requesting resources with batch scripts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a5115478b8e28ee",
   "metadata": {},
   "source": [
    "#SBATCH --account=stats_dept1\n",
    "#SBATCH --partition=standard\n",
    "\n",
    "#SBATCH --ntasks=10\n",
    "#SBATCH --nodes=10\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --cpus-per-task=16\n",
    "\n",
    "## 5GB/cpu is the basic share\n",
    "#SBATCH --mem-per-cpu=5GB\n",
    "\n",
    "## wall time days:hours:minutes:seconds\n",
    "#SBATCH --time=00-10:00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87472eb6a5ea2348",
   "metadata": {},
   "source": [
    "The above chunk of commands specifies the following:\n",
    "1. account and partition: typically PhD students in the Department of Statistics use this combination; if you were an undergrad, you may also have access to [an LSA account](https://arc.umich.edu/document/lsa-public-great-lakes-accounts/).\n",
    "2. `ntasks`, `nodes`, `ntasks-per-node`: task & node distributions. The above, and the simplest scheme is to parallel on n nodes, and submit one job on each node, in which case you configure the resources as above.\n",
    "3. `cpus-per-task`: Depends on whether each of the tasks is parallelized. If you need to parallelize within your code, e.g. an inference algorithm where you parallelize p-value calculations for each parameter, then you will need more than one cpu to do this. Otherwise, if each of your subtask on each node is a serial task, e.g. you have a simulation program, where you partition a total of 500 iterations into subtasks, each of size 1 simulation, and assign them to 500 nodes, but within the tasks, nothing is parallelized (in the p-value example, for each simulation, you construct p-values for each parameter sequentially, instead of simultaneously), then you only need 1 cpu for each node.\n",
    "4. `mem-per-cpu`: The RAM of each CPU. Oftentimes for programs used in our group, you rarely use up the set 5GB, something like 500MB should be sufficient. \n",
    "5. `time`: You may specify the program running time here. Keep in mind that if you don't know how long a large scale (e.g. 500 simulations) job will take, you may submit a small scale job (e.g. 1 simulation) and requesting much less resources (running time, cores, etc.) to have a sense for that, and then submit your large scale job while requesting an appropriate amount of resources.\n",
    "6. **Please always keep in mind that it is good practice to not request too much unnecessary resource when submitting a job, as your job's queuing time depend on your requested resource, and requesting unnecessary amount of cores/nodes/time could delay the running by even weeks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc7f471b29b3c8",
   "metadata": {},
   "source": [
    "# Activating the Virtual Environment for Cluster Jobs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a97cb92a020bbeb8",
   "metadata": {},
   "source": [
    "cd /home/yilingh/SI-Graphs\n",
    "source env3/bin/activate\n",
    "module load python/3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777805e8dc8040f",
   "metadata": {},
   "source": [
    "The above chunk of code has the following functionalities:\n",
    "1. Change the current operating directory to the directory of the git repository\n",
    "(and you may modify the path to your repo accordingly)\n",
    "2. Activate the virtual environment we created for the project, with necessary python packages installed\n",
    "3. Specify python as version 3.10. This is necessary to avoid version incompatibility for some projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb0d28b92f62d8",
   "metadata": {},
   "source": [
    "# Arranging Parallel Jobs (Please Modify this as Needed)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17e6d5a025d8b69c",
   "metadata": {},
   "source": [
    "start_end=(0 5 10 15 20 25 30 35 40 45 50)\n",
    "\n",
    "# Get the length of the array\n",
    "array_length=${#start_end[@]}\n",
    "\n",
    "# Loop through the array with an iteration index\n",
    "for ((i=0; i<array_length-1; i++)); do\n",
    "    # Your logic for each iteration here\n",
    "    python3 selectinf/Tests/nbd_simulation_vary_signal.py ${start_end[i]} ${start_end[i+1]} 0 16 &\n",
    "done\n",
    "wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2b77ba6b3ac78",
   "metadata": {},
   "source": [
    "The above chunk of code is only an easy way to parallel jobs, and you may find some more effective ways to do it. What is done essentially is, to partition 50 simulations into 10 blocks of 5. The vector `start_end` collects endpoints for simulation indices. Then we loop over the index array, and submit jobs for each segment defined by these indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73037e538f37cf",
   "metadata": {},
   "source": [
    "The line "
   ]
  },
  {
   "cell_type": "raw",
   "id": "14a2b824ac783bf6",
   "metadata": {},
   "source": [
    "python3 selectinf/Tests/nbd_simulation_vary_signal.py ${start_end[i]} ${start_end[i+1]} 0 16 &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582b37c0ee948eb",
   "metadata": {},
   "source": [
    "submits a task to a node, where the task is to use `python3` to run the script `selectinf/Tests/nbd_simulation_vary_signal.py`.\n",
    "\n",
    "Here, the script's `main()` function takes in 4 arguments from the input stream, i.e., \n",
    "1. starting simulation index\n",
    "2. ending simulation index\n",
    "3. 0 is an argument specifically used for this project, this may not be needed for other project\n",
    "4. 16 specifies we have 16 CPU cores for each node. This is needed for this particular project because I internally parallelized the inference algorithm over parameters. Again, if your method is purely serial, or something that does not need to be done sequentially (e.g. a joint MLE estimation approach), internal parallelization is not needed.\n",
    "\n",
    "The ampersand `&` at the end of the command is necessary for the simultaneous job submission to different nodes, as well as the `done` and `wait` commands. Removing these may cause problems in job submissions.\n",
    "\n",
    "To instantiate your understanding, I have attached the `main()` function for `nbd_simulation_vary_signal.py`, so that you know how to write a main function that takes in these 4 arguments accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5054e6bf852e43",
   "metadata": {},
   "source": [
    "```\n",
    "if __name__ == '__main__':\n",
    "    argv = sys.argv\n",
    "    # argv = [..., start, end, logic_tf, ncores]\n",
    "    start, end = int(argv[1]), int(argv[2])\n",
    "    logic_tf = int(argv[3])\n",
    "    ncores = int(argv[4])\n",
    "\n",
    "    nbd_simulations_vary_signal(range_=range(start, end), logic_tf=logic_tf,\n",
    "                                ncores=ncores,m=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d54033aeba0ce",
   "metadata": {},
   "source": [
    "`nbd_simulations_vary_signal()` is the simulation function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
