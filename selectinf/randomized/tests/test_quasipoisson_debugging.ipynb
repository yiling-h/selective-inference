{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nose.tools as nt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import regreg.api as rr\n",
    "\n",
    "from selectinf.randomized.group_lasso_query import (group_lasso,split_group_lasso)\n",
    "from selectinf.randomized.group_lasso_query_quasi import (group_lasso_quasi, split_group_lasso_quasi)\n",
    "\n",
    "# from selectinf.base import (selected_targets,selected_targets_quasi)\n",
    "from selectinf.randomized.tests.instance import (quasi_poisson_group_instance, poisson_group_instance)\n",
    "\n",
    "from selectinf.base import restricted_estimator\n",
    "from selectinf.randomized.tests.test_quasipoisson_group_lasso import calculate_F1_score, naive_inference, \\\n",
    "    randomization_inference, randomization_inference_poisson, data_splitting\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def test_comparison_quasipoisson_group_lasso_vary_s(n=500,\n",
    "                                          p=200,\n",
    "                                          signal_fac=0.1,\n",
    "                                          s=5,\n",
    "                                          sigma=2,\n",
    "                                          rho=0.3,\n",
    "                                          randomizer_scale=1.,\n",
    "                                          full_dispersion=True,\n",
    "                                          level=0.90,\n",
    "                                          iter=1):\n",
    "    \"\"\"\n",
    "    Compare to R randomized lasso\n",
    "    \"\"\"\n",
    "\n",
    "    # Operating characteristics\n",
    "    oper_char = {}\n",
    "    oper_char[\"sparsity size\"] = []\n",
    "    oper_char[\"coverage rate\"] = []\n",
    "    oper_char[\"avg length\"] = []\n",
    "    oper_char[\"method\"] = []\n",
    "    oper_char[\"F1 score\"] = []\n",
    "    #oper_char[\"runtime\"] = []\n",
    "\n",
    "    confint_df = pd.DataFrame()\n",
    "\n",
    "    for s in [5, 8, 10]:  # [0.01, 0.03, 0.06, 0.1]:\n",
    "        for i in range(iter):\n",
    "            # np.random.seed(i)\n",
    "\n",
    "            # inst = quasi_poisson_group_instance\n",
    "            inst_p = poisson_group_instance\n",
    "            const = group_lasso_quasi.quasipoisson\n",
    "            const_split = split_group_lasso_quasi.quasipoisson\n",
    "\n",
    "            signal = np.sqrt(signal_fac * 2 * np.log(p))\n",
    "            signal_str = str(np.round(signal, decimals=2))\n",
    "\n",
    "            while True:  # run until we get some selection\n",
    "                groups = np.arange(50).repeat(4)\n",
    "                \"\"\"\n",
    "                X, Y, beta = inst(n=n,\n",
    "                                  p=p,\n",
    "                                  signal=signal,\n",
    "                                  sgroup=s,\n",
    "                                  groups=groups,\n",
    "                                  ndiscrete=0,\n",
    "                                  nlevels=0,\n",
    "                                  sdiscrete=0,  # s-3, # How many discrete rvs are not null\n",
    "                                  equicorrelated=False,\n",
    "                                  rho=rho,\n",
    "                                  phi=1.5,\n",
    "                                  random_signs=True,\n",
    "                                  center=False,\n",
    "                                  scale=True)[:3]\n",
    "                                  \"\"\"\n",
    "                # print(X)\n",
    "\n",
    "                X, Y, beta = inst_p(n=n,\n",
    "                                  p=p,\n",
    "                                  signal=signal,\n",
    "                                  sgroup=s,\n",
    "                                  groups=groups,\n",
    "                                  ndiscrete=0,\n",
    "                                  nlevels=0,\n",
    "                                  sdiscrete=0,  # s-3, # How many discrete rvs are not null\n",
    "                                  equicorrelated=False,\n",
    "                                  rho=rho,\n",
    "                                  random_signs=True,\n",
    "                                  center=False,\n",
    "                                  scale=True)[:3]\n",
    "\n",
    "                n, p = X.shape\n",
    "\n",
    "                noselection = False  # flag for a certain method having an empty selected set\n",
    "\n",
    "                \"\"\"\n",
    "                if not noselection:\n",
    "                    # carving\n",
    "                    coverage_s, length_s, beta_target_s, nonzero_s, \\\n",
    "                    selection_idx_s, K, conf_low_s, conf_up_s = \\\n",
    "                        split_inference(X=X, Y=Y, n=n, p=p,\n",
    "                                        beta=beta, groups=groups, const=const_split,\n",
    "                                        proportion=0.5)\n",
    "\n",
    "                    noselection = (coverage_s is None)\n",
    "                    if noselection:\n",
    "                        print('No selection for carving')\n",
    "                \"\"\"\n",
    "                if not noselection:\n",
    "                    # MLE inference\n",
    "                    coverage, length, beta_target, nonzero, conf_low, conf_up = \\\n",
    "                        randomization_inference(X=X, Y=Y, n=n, p=p, #proportion=0.5,\n",
    "                                                beta=beta, groups=groups)\n",
    "\n",
    "                    noselection = (coverage is None)\n",
    "                    print(\"MLE inference noselection:\", noselection)\n",
    "\n",
    "\n",
    "                \"\"\"# Poisson inference, to be deleted\n",
    "                if not noselection:\n",
    "                    # MLE inference (Poisson)\n",
    "                    coverage_p, length_p, beta_target_p, nonzero_p, conf_low_p, conf_up_p = \\\n",
    "                        randomization_inference_poisson(X=X, Y=Y, n=n, p=p, #proportion=0.5,\n",
    "                                                        beta=beta, groups=groups)\n",
    "                    noselection = (coverage_p is None)\"\"\"\n",
    "                \"\"\"\n",
    "                # Poisson data splitting, to be deleted\n",
    "                if not noselection:\n",
    "                    # data splitting\n",
    "                    coverage_dsp, lengths_dsp, conf_low_dsp, conf_up_dsp = \\\n",
    "                        data_splitting_poisson(X=X, Y=Y, n=n, p=p, beta=beta, nonzero=nonzero_s,\n",
    "                                               subset_select=selection_idx_s, level=0.9)\n",
    "                    noselection = (coverage_dsp is None)\n",
    "                \"\"\"\n",
    "                if not noselection:\n",
    "                    # data splitting\n",
    "                    coverage_ds, lengths_ds, conf_low_ds, conf_up_ds, nonzero_ds, beta_target_ds = \\\n",
    "                        data_splitting(X=X, Y=Y, n=n, p=p, groups=groups, beta=beta,\n",
    "                                       proportion=0.5, level=0.9)\n",
    "                    noselection = (coverage_ds is None)\n",
    "                    print(\"data splitting noselection:\", noselection)\n",
    "\n",
    "                if not noselection:\n",
    "                    # naive inference\n",
    "                    coverage_naive, lengths_naive, nonzero_naive, conf_low_naive, conf_up_naive, \\\n",
    "                        beta_target_naive = \\\n",
    "                        naive_inference(X=X, Y=Y, groups=groups,\n",
    "                                        beta=beta, const=const,\n",
    "                                        n=n, level=level, nonzero_true=(beta != 0))\n",
    "                    noselection = (coverage_naive is None)\n",
    "                    print(\"naive inference noselection:\", noselection)\n",
    "\n",
    "                if not noselection:\n",
    "                    # F1 scores\n",
    "                    # F1_s = calculate_F1_score(beta, selection=nonzero_s)\n",
    "                    F1 = calculate_F1_score(beta, selection=nonzero)\n",
    "                    F1_ds = calculate_F1_score(beta, selection=nonzero_ds)\n",
    "                    F1_naive = calculate_F1_score(beta, selection=nonzero_naive)\n",
    "                    # F1_p = calculate_F1_score(beta, selection=nonzero_p)\n",
    "                    # F1_dsp = calculate_F1_score(beta, selection=nonzero_s)\n",
    "\n",
    "                    # MLE coverage\n",
    "                    oper_char[\"sparsity size\"].append(s)\n",
    "                    oper_char[\"coverage rate\"].append(np.mean(coverage))\n",
    "                    oper_char[\"avg length\"].append(np.mean(length))\n",
    "                    oper_char[\"F1 score\"].append(F1)\n",
    "                    oper_char[\"method\"].append('MLE')\n",
    "                    df_MLE = pd.concat([pd.DataFrame(np.ones(nonzero.sum()) * i),\n",
    "                                        pd.DataFrame(beta_target),\n",
    "                                        pd.DataFrame(conf_low),\n",
    "                                        pd.DataFrame(conf_up),\n",
    "                                        pd.DataFrame(beta[nonzero] != 0),\n",
    "                                        pd.DataFrame(np.ones(nonzero.sum()) * s),\n",
    "                                        pd.DataFrame(np.ones(nonzero.sum()) * F1),\n",
    "                                        pd.DataFrame([\"MLE\"] * nonzero.sum())\n",
    "                                        ], axis=1)\n",
    "                    confint_df = pd.concat([confint_df, df_MLE], axis=0)\n",
    "\n",
    "                    \"\"\"# Carving coverage\n",
    "                    oper_char[\"sparsity size\"].append(s)\n",
    "                    oper_char[\"coverage rate\"].append(np.mean(coverage_s))\n",
    "                    oper_char[\"avg length\"].append(np.mean(length_s))\n",
    "                    oper_char[\"F1 score\"].append(F1_s)\n",
    "                    oper_char[\"method\"].append('Carving')\n",
    "                    #oper_char[\"runtime\"].append(0)\n",
    "                    df_s = pd.concat([pd.DataFrame(np.ones(nonzero_s.sum()) * i),\n",
    "                                      pd.DataFrame(beta_target_s),\n",
    "                                      pd.DataFrame(conf_low_s),\n",
    "                                      pd.DataFrame(conf_up_s),\n",
    "                                      pd.DataFrame(beta[nonzero_s] != 0),\n",
    "                                      pd.DataFrame(np.ones(nonzero_s.sum()) * s),\n",
    "                                      pd.DataFrame(np.ones(nonzero_s.sum()) * F1_s),\n",
    "                                      pd.DataFrame([\"Carving\"] * nonzero_s.sum())\n",
    "                                      ], axis=1)\n",
    "                    confint_df = pd.concat([confint_df, df_s], axis=0)\"\"\"\n",
    "\n",
    "                    # MLE (Poisson) coverage\n",
    "                    \"\"\"oper_char[\"sparsity size\"].append(s)\n",
    "                    oper_char[\"coverage rate\"].append(np.mean(coverage_p))\n",
    "                    oper_char[\"avg length\"].append(np.mean(length_p))\n",
    "                    oper_char[\"F1 score\"].append(F1_p)\n",
    "                    oper_char[\"method\"].append('MLE (Poisson)')\n",
    "                    df_p = pd.concat([pd.DataFrame(np.ones(nonzero_p.sum()) * i),\n",
    "                                      pd.DataFrame(beta_target_p),\n",
    "                                      pd.DataFrame(conf_low_p),\n",
    "                                      pd.DataFrame(conf_up_p),\n",
    "                                      pd.DataFrame(beta[nonzero_p] != 0),\n",
    "                                      pd.DataFrame(np.ones(nonzero_p.sum()) * s),\n",
    "                                      pd.DataFrame(np.ones(nonzero_p.sum()) * F1_p),\n",
    "                                      pd.DataFrame([\"MLE (Poisson)\"] * nonzero_p.sum())\n",
    "                                      ], axis=1)\n",
    "                    confint_df = pd.concat([confint_df, df_p], axis=0)\"\"\"\n",
    "\n",
    "                    # Data splitting coverage\n",
    "                    oper_char[\"sparsity size\"].append(s)\n",
    "                    oper_char[\"coverage rate\"].append(np.mean(coverage_ds))\n",
    "                    oper_char[\"avg length\"].append(np.mean(lengths_ds))\n",
    "                    oper_char[\"F1 score\"].append(F1_ds)\n",
    "                    oper_char[\"method\"].append('Data splitting')\n",
    "                    df_ds = pd.concat([pd.DataFrame(np.ones(nonzero_ds.sum()) * i),\n",
    "                                       pd.DataFrame(beta_target_ds),\n",
    "                                       pd.DataFrame(conf_low_ds),\n",
    "                                       pd.DataFrame(conf_up_ds),\n",
    "                                       pd.DataFrame(beta[nonzero_ds] != 0),\n",
    "                                       pd.DataFrame(np.ones(nonzero_ds.sum()) * s),\n",
    "                                       pd.DataFrame(np.ones(nonzero_ds.sum()) * F1_ds),\n",
    "                                       pd.DataFrame([\"Data splitting\"] * nonzero_ds.sum())\n",
    "                                       ], axis=1)\n",
    "                    confint_df = pd.concat([confint_df, df_ds], axis=0)\n",
    "\n",
    "                    \"\"\"# Data splitting (poisson) coverage\n",
    "                    oper_char[\"sparsity size\"].append(s)\n",
    "                    oper_char[\"coverage rate\"].append(np.mean(coverage_dsp))\n",
    "                    oper_char[\"avg length\"].append(np.mean(lengths_dsp))\n",
    "                    oper_char[\"F1 score\"].append(F1_dsp)\n",
    "                    oper_char[\"method\"].append('Data splitting (Poisson)')\n",
    "                    df_dsp = pd.concat([pd.DataFrame(np.ones(nonzero_s.sum()) * i),\n",
    "                                       pd.DataFrame(beta_target_s),\n",
    "                                       pd.DataFrame(conf_low_dsp),\n",
    "                                       pd.DataFrame(conf_up_dsp),\n",
    "                                       pd.DataFrame(beta[nonzero_s] != 0),\n",
    "                                       pd.DataFrame(np.ones(nonzero_s.sum()) * s),\n",
    "                                       pd.DataFrame(np.ones(nonzero_s.sum()) * F1_dsp),\n",
    "                                       pd.DataFrame([\"Data splitting (Poisson)\"] * nonzero_s.sum())\n",
    "                                       ], axis=1)\n",
    "                    confint_df = pd.concat([confint_df, df_ds], axis=0)\"\"\"\n",
    "\n",
    "                    # Naive coverage\n",
    "                    oper_char[\"sparsity size\"].append(s)\n",
    "                    oper_char[\"coverage rate\"].append(np.mean(coverage_naive))\n",
    "                    oper_char[\"avg length\"].append(np.mean(lengths_naive))\n",
    "                    oper_char[\"F1 score\"].append(F1_naive)\n",
    "                    oper_char[\"method\"].append('Naive')\n",
    "                    df_naive = pd.concat([pd.DataFrame(np.ones(nonzero_naive.sum()) * i),\n",
    "                                          pd.DataFrame(beta_target_naive),\n",
    "                                          pd.DataFrame(conf_low_naive),\n",
    "                                          pd.DataFrame(conf_up_naive),\n",
    "                                          pd.DataFrame(beta[nonzero_naive] != 0),\n",
    "                                          pd.DataFrame(np.ones(nonzero_naive.sum()) * s),\n",
    "                                          pd.DataFrame(np.ones(nonzero_naive.sum()) * F1_naive),\n",
    "                                          pd.DataFrame([\"Naive\"] * nonzero_naive.sum())\n",
    "                                          ], axis=1)\n",
    "                    confint_df = pd.concat([confint_df, df_naive], axis=0)\n",
    "\n",
    "                    break  # Go to next iteration if we have some selection\n",
    "\n",
    "    oper_char_df = pd.DataFrame.from_dict(oper_char)\n",
    "    oper_char_df.to_csv('selectinf/randomized/tests/quasipois_vary_sparsity.csv', index=False)\n",
    "    colnames = ['Index'] + ['target'] + ['LCB'] + ['UCB'] + ['TP'] + ['sparsity size'] + ['F1'] + ['Method']\n",
    "    confint_df.columns = colnames\n",
    "    confint_df.to_csv('selectinf/randomized/tests/quasipois_CI_vary_sparsity.csv', index=False)\n",
    "\n",
    "    #sns.histplot(oper_char_df[\"sparsity size\"])\n",
    "    #plt.show()\n",
    "\n",
    "    print(\"Mean coverage rate/length:\")\n",
    "    print(oper_char_df.groupby(['sparsity size', 'method']).mean())\n",
    "\n",
    "    sns.boxplot(y=oper_char_df[\"coverage rate\"],\n",
    "                x=oper_char_df[\"sparsity size\"],\n",
    "                hue=oper_char_df[\"method\"],\n",
    "                orient=\"v\")\n",
    "    plt.show()\n",
    "\n",
    "    len_plot = sns.boxplot(y=oper_char_df[\"avg length\"],\n",
    "                           x=oper_char_df[\"sparsity size\"],\n",
    "                           hue=oper_char_df[\"method\"],\n",
    "                           showmeans=True,\n",
    "                           orient=\"v\")\n",
    "    len_plot.set_ylim(0, 8)\n",
    "    plt.show()\n",
    "\n",
    "    F1_plot = sns.boxplot(y=oper_char_df[\"F1 score\"],\n",
    "                          x=oper_char_df[\"sparsity size\"],\n",
    "                          hue=oper_char_df[\"method\"],\n",
    "                          showmeans=True,\n",
    "                          orient=\"v\")\n",
    "    F1_plot.set_ylim(0, 1)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(MLE) K estimated with full model\n",
      "H norm:  7.035075205121727\n",
      "K norm:  6.5355821643036105\n",
      "H-K norm:  2.9271783086208045\n",
      "Sigma_E norm:  8.31952993962497\n",
      "H^{-1} norm:  8.69927153632105\n",
      "H^{-1}-Sigma_E norm:  3.9137934569214274\n",
      "MLE inference noselection: False\n",
      "(Data Splitting) Selection done without carving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/regreg/smooth/glm.py:1060: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_terms = - coef * ((counts - 1) * np.log(counts))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting |E|: 0\n",
      "data splitting noselection: True\n",
      "(MLE) K estimated with full model\n",
      "H norm:  5.452799941628569\n",
      "K norm:  4.687063088179814\n",
      "H-K norm:  1.6466850985620463\n",
      "Sigma_E norm:  5.764316868404765\n",
      "H^{-1} norm:  6.4290015723893195\n",
      "H^{-1}-Sigma_E norm:  1.8547431487862562\n",
      "MLE inference noselection: False\n",
      "(Data Splitting) Selection done without carving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/regreg/smooth/glm.py:1060: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_terms = - coef * ((counts - 1) * np.log(counts))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting |E|: 4\n",
      "data splitting noselection: False\n",
      "(Naive) True E used\n",
      "Naive selection [False False False False False False False False  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True False False False False]\n",
      "H norm:  5.03049424643347\n",
      "K norm:  4.205131388732294\n",
      "H-K norm:  1.4914728606076266\n",
      "Sigma_E norm:  5.016413251400352\n",
      "H^{-1} norm:  5.542481657629109\n",
      "H^{-1}-Sigma_E norm:  1.549265700943577\n",
      "naive inference noselection: False\n",
      "(MLE) K estimated with full model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/regreg/smooth/glm.py:1060: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_terms = - coef * ((counts - 1) * np.log(counts))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H norm:  6.618034858764372\n",
      "K norm:  7.8056072725772285\n",
      "H-K norm:  3.1115822169936997\n",
      "Sigma_E norm:  9.15635134418219\n",
      "H^{-1} norm:  8.010286787541501\n",
      "H^{-1}-Sigma_E norm:  3.55835441192286\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/6t/y2p8qwk54f7fnkwds613w0040000gs/T/ipykernel_52800/801653391.py\", line 1, in <module>\n",
      "    test_comparison_quasipoisson_group_lasso_vary_s()\n",
      "  File \"/var/folders/6t/y2p8qwk54f7fnkwds613w0040000gs/T/ipykernel_52800/4291647449.py\", line 92, in test_comparison_quasipoisson_group_lasso_vary_s\n",
      "    randomization_inference(X=X, Y=Y, n=n, p=p, #proportion=0.5,\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/selectinf/randomized/tests/test_quasipoisson_group_lasso.py\", line 323, in randomization_inference\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/selectinf/randomized/tests/test_quasipoisson_group_lasso.py\", line 297, in solve_target_restricted\n",
      "    # For LASSO, this is the OLS solution on X_{E,U}\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/selectinf/base.py\", line 42, in restricted_estimator\n",
      "    beta_E = loss_restricted.solve(**solve_args)\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/regreg/problems/composite.py\", line 218, in solve\n",
      "    self.solver.fit(**fit_args)\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/regreg/algorithms.py\", line 131, in fit\n",
      "    proposed_coefs, proposed_smooth = self.backtrack(itercount)\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/regreg/algorithms.py\", line 257, in backtrack\n",
      "    proposed_grad = self.composite.smooth_objective(proposed_coefs, mode='grad')\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/regreg/smooth/__init__.py\", line 248, in smooth_objective\n",
      "    g = self.affine_transform.adjoint_map(g)\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/regreg/affine/__init__.py\", line 261, in adjoint_map\n",
      "    return np.dot(self.linear_operator.T, u)\n",
      "  File \"<__array_function__ internals>\", line 180, in dot\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1030, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 960, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 870, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 704, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/yilingh/Desktop/PhD/SI_Codes/selective-inference/env3/lib/python3.10/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "test_comparison_quasipoisson_group_lasso_vary_s()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
